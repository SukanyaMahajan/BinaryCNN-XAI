{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNQc5mx57APWULFeyOp7roP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DxED_OVJaeqn"},"outputs":[],"source":["import torch\n","from torch import nn\n","import os"]},{"cell_type":"markdown","source":["####Setting up device-agnostic code"],"metadata":{"id":"UlP4ppy-DzQI"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"id":"WiYT_JwGDvh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"metadata":{"id":"2szdKEKRECmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Posixpath module in Python- functions for manipulating file paths in a portable manner\n","#Posixpath is a child node of Path()\n","from pathlib import Path"],"metadata":{"id":"jYgcN2xOL9Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#setting up training and validation data paths\n","data_path = Path(\"/content/gdrive/MyDrive/\")\n","image_path = data_path / \"CottonData-StandardFormat\""],"metadata":{"id":"97pLkh_hHNmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 0. Dataset Visualization"],"metadata":{"id":"DI1WP76Hysxv"}},{"cell_type":"code","source":["from PIL import Image\n","import numpy as np\n","import os\n","\n","def create_image_collage(image_folder, collage_size, output_path):\n","    # Get all image files in the specified folder\n","    #image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg', 'gif'))]\n","\n","    # Select a fixed number of images (e.g., 100)\n","    num_images_to_select = 100\n","    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg', 'gif'))][:num_images_to_select]\n","\n","    # Shuffle the image files for randomness\n","    np.random.shuffle(image_files)\n","\n","    # Open and resize images to fit the collage\n","    images = [Image.open(os.path.join(image_folder, img)).resize(collage_size) for img in image_files]\n","\n","    # Calculate the dimensions of the collage\n","    collage_width = collage_size[0] * int(np.ceil(np.sqrt(len(images))))\n","    collage_height = collage_size[1] * int(np.ceil(len(images) / np.sqrt(len(images))))\n","\n","    # Create a blank canvas for the collage\n","    collage = Image.new('RGB', (collage_width, collage_height), (255, 255, 255))\n","\n","    # Paste each image onto the canvas\n","    x_offset, y_offset = 0, 0\n","    for img in images:\n","        collage.paste(img, (x_offset, y_offset))\n","        x_offset += collage_size[0]\n","        if x_offset >= collage_width:\n","            x_offset = 0\n","            y_offset += collage_size[1]\n","\n","    # Save the collage to the specified output path\n","    collage.save(output_path)\n","\n","# Example usage:\n","image_folder_path = \"path_of_validation_dataset_of_crop\"\n","collage_size = (100, 100)  # Adjust the size of each image tile\n","output_collage_path = \"path_of_output_tiled_dataset_of_crop.jpg\"\n","\n","create_image_collage(image_folder_path, collage_size, output_collage_path)"],"metadata":{"id":"1GPsLV-fzGLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import numpy as np\n","import os\n","\n","def create_image_collage(image_folder, collage_size, output_path):\n","    # Get all image files in the specified folder\n","    #image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg', 'gif'))]\n","\n","    # Select a fixed number of images (e.g., 100)\n","    num_images_to_select = 100\n","    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg', 'gif'))][:num_images_to_select]\n","\n","\n","    # Shuffle the image files for randomness\n","    np.random.shuffle(image_files)\n","\n","    # Open and resize images to fit the collage\n","    images = [Image.open(os.path.join(image_folder, img)).resize(collage_size) for img in image_files]\n","\n","    # Calculate the dimensions of the collage\n","    collage_width = collage_size[0] * int(np.ceil(np.sqrt(len(images))))\n","    collage_height = collage_size[1] * int(np.ceil(len(images) / np.sqrt(len(images))))\n","\n","    # Create a blank canvas for the collage\n","    collage = Image.new('RGB', (collage_width, collage_height), (255, 255, 255))\n","\n","    # Paste each image onto the canvas\n","    x_offset, y_offset = 0, 0\n","    for img in images:\n","        collage.paste(img, (x_offset, y_offset))\n","        x_offset += collage_size[0]\n","        if x_offset >= collage_width:\n","            x_offset = 0\n","            y_offset += collage_size[1]\n","\n","    # Save the collage to the specified output path\n","    collage.save(output_path)\n","\n","# Example usage:\n","image_folder_path = \"path_of_validation_dataset_of_weed\"\n","collage_size = (100,100)  # Adjust the size of each image tile\n","output_collage_path = \"path_of_output_tiled_dataset_of_weed.jpg\"\n","\n","create_image_collage(image_folder_path, collage_size, output_collage_path)"],"metadata":{"id":"YK0mb7Hn1wro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Data preparation"],"metadata":{"id":"gcD_vtcdUBWr"}},{"cell_type":"code","source":["def walk_through_dir(dir_path):\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"there are {len(dirnames)} directories and {len(filenames)} images in {dirpath}.\")\n"],"metadata":{"id":"HZQfdTndPmUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["walk_through_dir(image_path)"],"metadata":{"id":"qW3xKHp3P--w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = image_path / \"Train\"\n","val_dir = image_path / \"Validation\"\n","test_dir = image_path/ \"Test\"\n","train_dir, val_dir, test_dir"],"metadata":{"id":"JkaoRATQJwIu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.listdir(image_path)"],"metadata":{"id":"DJx2-7sCOpLj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1 Visualizing a single image"],"metadata":{"id":"2hryg8oFTh8x"}},{"cell_type":"code","source":["import random\n","from PIL import Image\n","\n","#set seed\n","random.seed(42)\n","\n","# 1. get all image paths (* means any combination)\n","image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n","\n","# 2. get random image path\n","random_img_path = random.choice(image_path_list)\n","\n","# 3. get image class from path name\n","image_class = random_img_path.parent.stem\n","\n","# 4. open image\n","img = Image.open(random_img_path)\n","\n","# 5. print metadata\n","print(f\"Random image path: {random_img_path}\")\n","print(f\"Image class: {image_class}\")\n","print(f\"Image height: {img.height}\")\n","print(f\"Image width: {img.width}\")\n","img"],"metadata":{"id":"PkWHIRwdLdqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","# visualizing with matplotlib\n","# convert the img to numpy for visualizing in matplotlib\n","\n","# turn the img in an array\n","img_as_array = np.asarray(img)\n","\n","# plot img with matplotlib\n","plt.figure(figsize=(6,6)) #6inch by 6inch\n","plt.imshow(img_as_array)\n","plt.title(f\"Image class: {image_class}, Image shape: {img_as_array.shape} -> [H, W, CC]\")\n","plt.axis(False);"],"metadata":{"id":"XkqsoL5_OefF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 Transforming data\n","Before we can use our image data with PyTorch we need to:\n","\n","1.Turn it into tensors (numerical representations of our images).\n","\n","2.Turn it into a torch.utils.data.Dataset and subsequently a torch.utils.data.DataLoader, we'll call these Dataset and DataLoader for short."],"metadata":{"id":"9gEDToqCXFyI"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"metadata":{"id":"AvqurWa2VFVD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.Resize the images using transforms.Resize() (from about 512x512 to 128X128, the same shape as the images on the CNN Explainer website).\n","\n","2.Flip our images randomly on the horizontal using transforms.RandomHorizontalFlip() (this could be considered a form of data augmentation because it will artificially change our image data).\n","\n","3.Turn our images from a PIL image to a PyTorch tensor using transforms.ToTensor()."],"metadata":{"id":"pdxq0AGbfvF1"}},{"cell_type":"code","source":["# write transform for image\n","data_transform = transforms.Compose([\n","    # Resize the image to 128x128\n","    transforms.Resize(size=(128,128), antialias=None),\n","    # flip the images randomly on the horizontal\n","    transforms.RandomHorizontalFlip(p=0.5), #p=prob of flip\n","    # turn the image into a tensor\n","    transforms.ToTensor(),# this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n","    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n","])"],"metadata":{"id":"HB7SJbxUgFSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# applying the above transforms on various images.\n","def plot_transformed_images(image_path, transform, n=3, seed=42):\n","\n","  \"\"\"Plots a series of random images from image_paths.\n","\n","    Will open n image paths from image_paths, transform them\n","    with transform and plot them side by side.\n","\n","    Args:\n","        image_paths (list): List of target image paths.\n","        transform (PyTorch Transforms): Transforms to apply to images.\n","        n (int, optional): Number of images to plot. Defaults to 3.\n","        seed (int, optional): Random seed for the random generator. Defaults to 42.\n","    \"\"\"\n","  #random.seed(seed)\n","  random_image_paths = random.sample(image_path, k=n)\n","  for image_path in random_image_paths:\n","    with Image.open(image_path) as f:\n","      fig, ax = plt.subplots(1,2)\n","      ax[0].imshow(f)\n","      ax[0].set_title(f\"original \\nsize: {f.size}\")\n","      ax[0].axis(\"off\")\n","\n","      # Transform and plot image\n","      # Note: permute() will change shape of image to suit matplotlib\n","      # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n","      transformed_image = transform(f).permute(1,2,0)\n","      ax[1].imshow(transformed_image)\n","      ax[1].set_title(f\"Tranformed \\nsize: {transformed_image.shape}\")\n","      ax[1].axis(\"off\")\n","\n","      fig.suptitle(f\"class: {image_path.parent.stem}\", fontsize=16)\n","plot_transformed_images(image_path_list,\n","                        transform=data_transform,\n","                        n=3)\n"],"metadata":{"id":"djgP9GVMg_g3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Loading image data using ImageFolder\n","Alright, time to turn our image data into a Dataset capable of being used with PyTorch."],"metadata":{"id":"DTHmYptymsfu"}},{"cell_type":"code","source":["# use ImageFolder to create dataset\n","from torchvision import datasets\n","from torchvision.datasets import ImageFolder\n","train_data = datasets.ImageFolder(root=train_dir,\n","                                  transform=data_transform,\n","                                  target_transform=None)\n","val_data = datasets.ImageFolder(root=val_dir,\n","                                transform=data_transform)\n","print(f\"Train data :\\n{train_data}\\nVal data :\\n{val_data}\")"],"metadata":{"id":"QmuD_-d9kfMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = datasets.ImageFolder(root=test_dir,\n","                                 transform=data_transform)\n","print(f\"Test data: \\n{test_data}\")"],"metadata":{"id":"mTbIYR21f_el"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get class names as a list\n","class_names = train_data.classes\n","class_names"],"metadata":{"id":"fK4yr8v-oIsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can also get class names as a dict\n","class_dict = train_data.class_to_idx\n","class_dict"],"metadata":{"id":"rSy6KkD80Or6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the length\n","len(train_data), len(val_data)"],"metadata":{"id":"h_zIT9xD0W1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = train_data [0][0], train_data[0][1]\n","print(f\"image tensor: {img}\")\n","print(f\"image shape: {img.shape}\")\n","print(f\"image datatype: {img.dtype}\")\n","print(f\"image label: {label}\")\n","print(f\"label datatype: {type(label)}\")"],"metadata":{"id":"WiUok-EJ0gjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rearrange the order of dimensions\n","img_permute = img.permute(1, 2, 0)\n","\n","# Print out different shapes (before and after permute)\n","print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n","print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n","\n","# Plot the image\n","plt.figure(figsize=(10, 7))\n","plt.imshow(img.permute(1, 2, 0))\n","plt.axis(\"off\")\n","plt.title(class_names[label], fontsize=14);"],"metadata":{"id":"2le3fIhE1zPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.Turn loaded images into DataLoader's\n","Turning our Dataset's into DataLoader's makes them iterable so a model can go through learn the relationships between samples and targets (features and labels)."],"metadata":{"id":"p4ZcYZwC2Ihs"}},{"cell_type":"code","source":["# Turn train and val Datasets into DataLoaders\n","from torch.utils.data import DataLoader\n","BATCH_SIZE = 32\n","NUM_WORKERS = os.cpu_count()\n","train_dataloader = DataLoader(dataset=train_data,\n","                              batch_size=BATCH_SIZE, # how many samples per batch?\n","                              num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n","                              shuffle=True) # shuffle the data?\n","\n","val_dataloader = DataLoader(dataset=val_data,\n","                             batch_size=BATCH_SIZE,\n","                             num_workers=NUM_WORKERS,\n","                             shuffle=False) # don't usually need to shuffle testing data\n","\n","train_dataloader, val_dataloader"],"metadata":{"id":"Z__aJFs72ruP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataloader = DataLoader(dataset=test_data,\n","                             batch_size=BATCH_SIZE,\n","                             num_workers=NUM_WORKERS,\n","                             shuffle=False)\n","test_dataloader"],"metadata":{"id":"O0a4pxHqfOA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = next(iter(train_dataloader))\n","\n","# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n","print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n","print(f\"Label shape: {label.shape}\")"],"metadata":{"id":"LUDwR21s2-_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Other forms of transforms (Data Augmentation)\n","Data augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.\n","\n","You usually don't perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set."],"metadata":{"id":"j6rskRRx4Zkc"}},{"cell_type":"code","source":["from torchvision import transforms\n","\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n","    transforms.ToTensor()\n","])\n","# Don't need to perform augmentation on the val data\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"pkAsYyfg3Gnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get all image paths\n","image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n","\n","#plot random image\n","plot_transformed_images(\n","    image_path=image_path_list,\n","    transform=train_transforms,\n","    n=5,\n","    seed=None\n",")"],"metadata":{"id":"o450xjJP8X3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating transforms and loading data for Model 0\n","\n","we will use earlier transforms and not create a new transform.Also using previous dataloaders."],"metadata":{"id":"5q2DroU-tHil"}},{"cell_type":"markdown","source":["### 4.Create TinyVGG model class"],"metadata":{"id":"CFb8nCm4wxTD"}},{"cell_type":"code","source":["class TinyVGG(nn.Module):\n","  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","    super().__init__()\n","    self.conv_block_1 = nn.Sequential(\n","        nn.Conv2d(in_channels=input_shape,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(in_channels=hidden_units,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2,\n","                     stride=2)\n","    )\n","    self.conv_block_2 = nn.Sequential(\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(in_features = hidden_units*32*32,\n","                  out_features=output_shape)\n","    )\n","  def forward(self, x:torch.Tensor):\n","    x = self.conv_block_1(x)\n","    x = self.conv_block_2(x)\n","    x = self.classifier(x)\n","    return x\n","\n","torch.manual_seed(42)\n","model_BCE_20epochs = TinyVGG(input_shape=3,\n","                  hidden_units=10,\n","                  output_shape=len(train_data.classes)).to(device)\n","model_BCE_20epochs"],"metadata":{"id":"UbGhGQw9wMKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.1 Try a forward pass on a single image(to test the model)"],"metadata":{"id":"Q9RhI-Ujznd-"}},{"cell_type":"code","source":["# 1. Get a batch of images and labels from the DataLoader\n","img_batch, label_batch = next(iter(train_dataloader))\n","\n","# 2. Get a single image from the batch and\n","#unsqueeze the image so its shape fits the model\n","img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n","print(f\"single image shape: {img_single.shape}\")\n","\n","# 3. Perform a forward pass on the single image\n","model_BCE_20epochs.eval()\n","with torch.inference_mode():\n","  pred = model_BCE_20epochs(img_single.to(device))\n","\n","# 4. Print what's happening\n","# convert model logits -> pred probs -> pred labels\n","print(f\"output logits: {pred}\")\n","print(f\"output prediction probabilities: {torch.sigmoid(pred)}\")\n","print(f\"output prediction label: {torch.round(torch.sigmoid(pred))}\")\n","print(f\"actual label: {label_single}\")"],"metadata":{"id":"pNn3SVsSzfoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Using torchinfo to get the idea of the shapes going through our model"],"metadata":{"id":"IJHGmKnKlVjx"}},{"cell_type":"code","source":["# Install torchinfo if it's not available, import it if it is\n","try:\n","    import torchinfo\n","except:\n","    !pip install torchinfo\n","    import torchinfo\n","\n","from torchinfo import summary\n","summary(model_BCE_20epochs, input_size=[1, 3, 128, 128]) # do a test pass through of an example input size"],"metadata":{"id":"nwXqu8b5lU-y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Train and Test loop functions\n","Now let's make some training and test loop functions to train our model on the training data and evaluate our model on the validation data.\n","\n","1. train_step() - takes in a model, a DataLoader, a loss function and an optimizer and trains the model on the DataLoader.\n","\n","2. test_step() - takes in a model, a DataLoader and a loss function and evaluates the model on the DataLoader.\n","\n","3. train() - performs 1. and 2. together for a given number of epochs and returns a results dictionary."],"metadata":{"id":"vMhbG7mql51g"}},{"cell_type":"code","source":["def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer):\n","  # Put model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss, train_acc = 0, 0\n","\n","  # Loop through data loader data batches\n","  for batch, (X,y) in enumerate(dataloader):\n","    #send data to target device\n","\n","    X,y = X.to(device), y.to(device)\n","\n","    #print(f\"y shape: {y.shape}\")\n","    y = y.view(-1,1).float()\n","    #print(f\"y shape after view: {y.shape}\")\n","\n","    # 1. Forward pass\n","    y_pred = model(X)\n","    #print(f\"y_pred shape: {y_pred.shape}\")\n","\n","    # 2. Calculate and accumulate loss\n","    loss = loss_fn(y_pred, y)\n","    train_loss += loss.item()\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backward\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    # Calculate and accumulate accuracy metric across all batches\n","    y_pred_class = torch.round(torch.sigmoid(y_pred))\n","    train_acc += (y_pred_class ==y).sum().item()/len(y_pred)\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss/len(dataloader)\n","  train_acc = train_acc/len(dataloader)\n","  return train_loss, train_acc"],"metadata":{"id":"2K5wIqsxl9Pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The main difference here will be the test_step() won't take an optimizer\n","#and therefore won't perform gradient descent.\n","def val_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module):\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    val_loss, val_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","\n","            X, y = X.to(device), y.to(device)\n","            y = y.view(-1,1).float()\n","\n","            # 1. Forward pass\n","            val_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(val_pred_logits, y)\n","            val_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            val_pred_labels = torch.round(torch.sigmoid(val_pred_logits))\n","            val_acc += ((val_pred_labels == y).sum().item()/len(val_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    val_loss = val_loss / len(dataloader)\n","    val_acc = val_acc / len(dataloader)\n","    return val_loss, val_acc"],"metadata":{"id":"eECWwRhjVyEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Creating a train() function to combine train_step() and test_step()\n","\n","Specificially, it'll:\n","\n","1. Take in a model, a DataLoader for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.\n","\n","2.  Create an empty results dictionary for train_loss, train_acc, test_loss and test_acc values (we can fill this up as training goes on).\n","\n","3.Loop through the training and test step functions for a number of epochs.\n","\n","4. Print out what's happening at the end of each epoch.\n","\n","5. Update the empty results dictionary with the updated metrics each epoch.\n","\n","6. Return the filled"],"metadata":{"id":"MLZvDQ0EWxtL"}},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","# 1. Take in various parameters required for training and test steps\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module = nn.BCEWithLogitsLoss(),\n","          epochs: int = 5):\n","  # 2. Create empty results dictionary\n","  results = {\"train_loss\": [],\n","             \"train_acc\": [],\n","             \"val_loss\": [],\n","             \"val_acc\": []}\n","\n","  # 3. Loop through training and validation steps for a number of epochs\n","  for epoch in tqdm(range(epochs)):\n","    train_loss, train_acc = train_step(model=model,\n","                                       dataloader=train_dataloader,\n","                                       loss_fn=loss_fn,\n","                                       optimizer=optimizer)\n","    val_loss, val_acc = val_step(model=model,\n","                                 dataloader=val_dataloader,\n","                                 loss_fn=loss_fn)\n","    # 4. Print out what is happening\n","    print(\n","        f\"Epochs:{epoch+1}\",\n","        f\"train_loss: {train_loss:.4f}\",\n","        f\"train_acc: {train_acc:.4f}\",\n","        f\"val_loss: {val_loss:.4f}\",\n","        f\"val_acc: {val_acc:.4f}\"\n","    )\n","\n","    #  5. update dictionary\n","    results[\"train_loss\"].append(train_loss)\n","    results[\"train_acc\"].append(train_acc)\n","    results[\"val_loss\"].append(val_loss)\n","    results[\"val_acc\"].append(val_acc)\n","\n","  # 6. return the filled results at the end of the epochs\n","  return results"],"metadata":{"id":"Tv9Kd3mYW0Aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.1 Train and Evaluate model 0"],"metadata":{"id":"KMqsUWJfakPx"}},{"cell_type":"code","source":["# Set random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","NUM_EPOCHS = 20\n","\n","# Recreate an instance of TinyVGG\n","model_BCE_20epochs = TinyVGG(input_shape=3, # number of color channels (3 for RGB)\n","                            hidden_units=10,\n","                            output_shape=1).to(device)\n","\n","# Setup loss function and optimizer\n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(params=model_BCE_20epochs.parameters(), lr=0.01)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Train model_BCE_8epochs\n","model_BCE_20epochs_results = train(model=model_BCE_20epochs,\n","                                  train_dataloader=train_dataloader,\n","                                  test_dataloader=val_dataloader,\n","                                  optimizer=optimizer,\n","                                  loss_fn=loss_fn,\n","                                  epochs=NUM_EPOCHS)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")"],"metadata":{"id":"vQ3YKoIxaJRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_BCE_20epochs_results.keys()"],"metadata":{"id":"59oOauGARvsz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8. Saving the Model"],"metadata":{"id":"-bhEf21xiYsD"}},{"cell_type":"code","source":["from pathlib import Path\n","\n","#create model directory path\n","MODEL_PATH = Path(\"path_of_CNN_model\")\n","MODEL_PATH.mkdir(parents=True, #creating dir if it doesn't exist\n","                 exist_ok=True)\n","\n","#create model save\n","MODEL_NAME = \"CNN_crop_weed_data_model_BCE_20epochs.pth\" #.pth/.pt for pytorch\n","MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n","\n","#save the model state dict\n","#We'll call torch.save(obj, f)\n","#where obj is the target model's state_dict() and f is the filename of where to save the model.\n","print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n","torch.save(obj=model_BCE_20epochs.state_dict(),\n","           f=MODEL_SAVE_PATH) #f is the file path i.e MODEL_SAVE_PATH"],"metadata":{"id":"QYSeqem7awdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9. Loading a saved PyTorch model's state_dict()\n","\n","Since we've now got a saved model state_dict() at models/01_pytorch_workflow_model_0.pth we can now load it in using torch.nn.Module.load_state_dict(torch.load(f)) where f is the filepath of our saved model state_dict().\n","\n","Why call torch.load() inside torch.nn.Module.load_state_dict()?\n","\n","Because we only saved the model's state_dict() which is a dictionary of learned parameters and not the entire model, we first have to load the state_dict() with torch.load() and then pass that state_dict() to a new instance of our model (which is a subclass of nn.Module)."],"metadata":{"id":"ZJ_VhJVZ2WIa"}},{"cell_type":"code","source":["# create a new instance\n","# Instantiate a new instance of our model (this will be instantiated with random weights)\n","torch.manual_seed(42)\n","\n","loaded_model_BCE_20epochs = TinyVGG(input_shape=3,\n","                                   hidden_units=10,\n","                                   output_shape=1).to(device)\n","# load in the save state_dict()\n","loaded_model_BCE_20epochs.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) #f = file that we want to load"],"metadata":{"id":"mtwP6nIMa3C5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10. Plot the loss curves of Model 0\n","Loss curves show the model's results over time.\n","\n","And they're a great way to see how your model performs on different datasets (e.g. training and val)."],"metadata":{"id":"MLz9KVtK0MFD"}},{"cell_type":"code","source":["def plot_loss_curves(results: dict[str, list[float]]):\n","  \"\"\"Plots training curves of a result dictionary.\n","  \"\"\"\n","  # Get the loss values of the results dictionary(training and val)\n","  loss = results['train_loss']\n","  val_loss = results['val_loss']\n","\n","  # Get the accuracy values of the results dictionary(training and val)\n","  accuracy = results['train_acc']\n","  val_accuracy = results['val_acc']\n","\n","  # Figure out how many epochs there were\n","  epochs = range(1,21,1) #range(len(results['train_loss']))\n","\n","  # Setup a plot\n","  plt.figure(figsize=(14,8))\n","  x_ticks = np.arange(1, 21 , step=1)\n","\n","  # Plot loss\n","  plt.plot(x_ticks, loss, label= 'train_loss')\n","  plt.plot(x_ticks, val_loss, label= 'val_loss')\n","  plt.title(\"Model Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Loss\")\n","  plt.xticks(x_ticks)\n","  plt.legend()\n","\n","  # Plot accuracy\n","  plt.figure(figsize=(14,8))\n","  plt.plot(x_ticks, accuracy, label='train_accuracy')\n","  plt.plot(x_ticks, val_accuracy, label='val_accuracy')\n","  plt.title('Model Accuracy')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Accuracy')\n","  plt.xticks(x_ticks)\n","  plt.legend();"],"metadata":{"id":"B1hQYDCr0bqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss_curves(model_BCE_20epochs_results)"],"metadata":{"id":"FowXpW993J20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","model_BCE_20epochs_df = pd.DataFrame(model_BCE_20epochs_results)\n","model_BCE_20epochs_df"],"metadata":{"id":"pgPHQYnC7sNc"},"execution_count":null,"outputs":[]},{"source":["from matplotlib import pyplot as plt\n","x_ticks = np.arange(0, 21 , step=1)\n","model_BCE_20epochs_df['val_acc'].plot(kind='line', figsize=(8, 4), title='val_acc')\n","plt.gca().spines[['top', 'right']].set_visible(False)\n","plt.xlabel(\"Epochs\")\n","plt.xticks(x_ticks)\n","plt.ylabel(\"Accuracy\")"],"cell_type":"code","metadata":{"id":"0kNTOBap3auH"},"execution_count":null,"outputs":[]},{"source":["from matplotlib import pyplot as plt\n","x_ticks = np.arange(0, 21 , step=1)\n","model_BCE_20epochs_df['val_loss'].plot(kind='line', figsize=(8, 4), title='val_loss')\n","plt.gca().spines[['top', 'right']].set_visible(False)\n","plt.xlabel(\"Epochs\")\n","plt.xticks(x_ticks)\n","plt.ylabel(\"Accuracy\")"],"cell_type":"code","metadata":{"id":"uLuElpmi3Zn-"},"execution_count":null,"outputs":[]},{"source":["from matplotlib import pyplot as plt\n","x_ticks = np.arange(0, 21 , step=1)\n","model_BCE_20epochs_df['train_acc'].plot(kind='line', figsize=(8, 4), title='train_acc')\n","plt.gca().spines[['top', 'right']].set_visible(False)\n","plt.xlabel(\"Epochs\")\n","plt.xticks(x_ticks)\n","plt.ylabel(\"Accuracy\")"],"cell_type":"code","metadata":{"id":"eUBe-HtQ3Yfk"},"execution_count":null,"outputs":[]},{"source":["from matplotlib import pyplot as plt\n","model_BCE_20epochs_df['train_loss'].plot(kind='line', figsize=(8, 4), title='train_loss')\n","x_ticks = np.arange(0, 21 , step=1)\n","plt.gca().spines[['top', 'right']].set_visible(False)\n","plt.xlabel(\"Epochs\")\n","plt.xticks(x_ticks)\n","plt.ylabel(\"Accuracy\")"],"cell_type":"code","metadata":{"id":"UJ0Pw3fm3W7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# displaying the Dataframe\n","print('Dataframe:\\n', model_BCE_20epochs_df)"],"metadata":{"id":"QUKlZ-yN7ufF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the data as CSV file\n","#model_BCE_20epochs_df_csv = model_BCE_20epochs_df.to_csv('BCEmodel_20 epochs.csv', index=True) #insert in word"],"metadata":{"id":"IWZsHe3cBCbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","\n","# Download helper functions from Learn PyTorch repo (if not already downloaded)\n","if os.path.isfile(\"helper_function_script_at_given_path.py\"):\n","  print(\"helper_functions.py already exists, skipping download\")\n","else:\n","  print(\"Downloading helper_functions.py\")\n","  os.chdir(\"given_path_for_helper_function_script\")\n","  # Note: you need the \"raw\" GitHub URL for this to work\n","  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","  with open(\"helper_functions.py\", \"wb\") as f:\n","    f.write(request.content)"],"metadata":{"id":"F83TdnZBTlG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from helper_functions import accuracy_fn"],"metadata":{"id":"kBkoVzSCVST7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAFe3S5d3NVp"},"outputs":[],"source":["torch.manual_seed(42)\n","def eval_model(model: torch.nn.Module,\n","              data_loader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              accuracy_fn,\n","              ):\n","  \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n","\n","  Args:\n","      model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n","      data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n","      loss_fn (torch.nn.Module): The loss function of model.\n","      accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n","\n","  Returns:\n","      (dict): Results of model making predictions on data_loader.\n","  \"\"\"\n","  loss, acc = 0,0\n","  model.eval()\n","  with torch.inference_mode():\n","    for X,y in data_loader:\n","      X,y = X.to(device), y.to(device)\n","      # make predictions with the model\n","      y_pred = model(X)\n","      y_pred_label = torch.round(torch.sigmoid(y_pred))\n","      y=y.view(-1,1).float()\n","\n","      # accumulate the loss and accuracy values per batch\n","      loss += loss_fn(y_pred, y)\n","      acc += accuracy_fn(y_true=y,\n","                         y_pred=y_pred_label)\n","      #print(f\"accuracy: {acc}, loss: {loss}\")\n","      # for accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n","    loss /= len(data_loader)\n","    acc /= len(data_loader)\n","\n","  return {\"model_name\": model.__class__.__name__,# only works when model was created with a class\n","          \"model_loss\": loss.item(),\n","          \"model_acc\": acc}\n","# calculate model 0 results on test dataset\n","model_BCE_20epochs_eval_results = eval_model(model=model_BCE_20epochs,\n","                                            data_loader=val_dataloader,\n","                                            loss_fn=loss_fn,\n","                                            accuracy_fn=accuracy_fn)\n","model_BCE_20epochs_eval_results"]},{"cell_type":"markdown","source":["### 11. Confusion Matrix with BCEWithLogitsLoss"],"metadata":{"id":"ZhBd5pQPU3Bk"}},{"cell_type":"code","source":["#import tqdm for progress bar\n","from tqdm.auto import tqdm\n","\n","# 1. make predictions with trained model\n","y_pred_label = []\n","y_true_label = []\n","model_BCE_20epochs.eval()\n","with torch.inference_mode():\n","  for X, y in tqdm(val_dataloader, desc='Making predictions...'):\n","    # send data and targets to target device\n","    X, y = X.to(device), y.to(device)\n","    # do the forward pass\n","    y_logit = model_BCE_20epochs(X)\n","    # turn predictions from logits -> prediction probabilities -> prediction labels\n","    y_pred = torch.round(torch.sigmoid(y_logit))\n","    # put predictions on CPU for evaluation\n","    y_pred_label.append(y_pred.cpu())\n","    # get true evaluation\n","    y_true_label.append(y.cpu()) # y is label"],"metadata":{"id":"Q8f7akUXauFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate list of predictions into a tensor\n","#print(y_pred_label)\n","#print(y_true_label)\n","y_true_tensor = torch.cat(y_true_label)\n","y_pred_tensor = torch.cat(y_pred_label).squeeze(dim=1)\n","y_pred_tensor.shape, y_true_tensor.shape\n"],"metadata":{"id":"iXxT0UnvmEg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See if torchmetrics exists, if not, install it\n","try:\n","    import torchmetrics, mlxtend\n","    print(f\"mlxtend version: {mlxtend.__version__}\")\n","    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n","except:\n","    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n","    import torchmetrics, mlxtend\n","    print(f\"mlxtend version: {mlxtend.__version__}\")"],"metadata":{"id":"PbiJ2JBRVf_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import mlxtend upgraded version\n","import mlxtend\n","print(mlxtend.__version__)\n","assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"],"metadata":{"id":"17ZA1gAGVpaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names #y_pred_label"],"metadata":{"id":"xOkxI6xXVvPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchmetrics import ConfusionMatrix\n","from mlxtend.plotting import plot_confusion_matrix\n","\n","# 2. Setup confusion matrix instance and compare predictions to targets\n","confmat = ConfusionMatrix(num_classes=2, task='BINARY')\n","confmat_tensor = confmat(preds = y_pred_tensor,\n","                         target = y_true_tensor)\n","\n","# plot the confusion matrix\n","fig, ax = plot_confusion_matrix(\n","    conf_mat = confmat_tensor.numpy(), # matplotlib likes working with NumPy\n","    class_names = class_names, # turn the row and column labels into class names\n","    figsize=(5,5),\n","    cmap=plt.get_cmap('PiYG')\n",");"],"metadata":{"id":"Kmg8YT7pV877"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_BCE_20epochs.to(device)"],"metadata":{"id":"FIhnMCAca_ha"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 12. Test Set EVALUATE\n","Now to evaluate our saved and loaded model on test_dataloader, let's perform inference with it (make predictions) on the test dataset."],"metadata":{"id":"Hzhlgulw7aN4"}},{"cell_type":"code","source":["# Evaluate the test set\n","torch.manual_seed(42)\n","\n","loaded_model_BCE_20epochs_results = eval_model(\n","    model=loaded_model_BCE_20epochs,\n","    data_loader=test_dataloader,\n","    loss_fn=loss_fn,\n","    accuracy_fn=accuracy_fn\n",")\n","loaded_model_BCE_20epochs_results"],"metadata":{"id":"0V2R3To2Q3vD"},"execution_count":null,"outputs":[]}]}