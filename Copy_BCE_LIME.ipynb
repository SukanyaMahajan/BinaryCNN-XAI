{"cells":[{"cell_type":"markdown","metadata":{"id":"rVwle13FRUmC"},"source":["## 1. Importing important libraries for LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGimMHKNRckC"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch.nn as nn\n","import numpy as np\n","import os,json\n","\n","import torch\n","from torchvision import models, transforms\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder"]},{"cell_type":"markdown","metadata":{"id":"xvt3Z4m8XJd9"},"source":["## Loading the saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0p30LJv8W__6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6u2lZIa9Z3oq"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbGhGQw9wMKG"},"outputs":[],"source":["class TinyVGG(nn.Module):\n","  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","    super().__init__()\n","    self.conv_block_1 = nn.Sequential(\n","        nn.Conv2d(in_channels=input_shape,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(in_channels=hidden_units,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2,\n","                     stride=2)\n","    )\n","    self.conv_block_2 = nn.Sequential(\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(in_features = hidden_units*32*32,\n","                  out_features=output_shape)\n","    )\n","  def forward(self, x:torch.Tensor):\n","    x = self.conv_block_1(x)\n","    x = self.conv_block_2(x)\n","    x = self.classifier(x)\n","    return x\n","\n","torch.manual_seed(42)\n","model_BCE_20epochs = TinyVGG(input_shape=3,\n","                             hidden_units=10,\n","                             output_shape=1).to(device)\n","model_BCE_20epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUD_Pnn4XhAg"},"outputs":[],"source":["#BCE_CNNmodel = torch.load(f=\"path_of_your_saved_CNN_model.pth\", map_location=torch.device(\"cpu\"))\n","#BCE_CNNmodel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRrKCseDYaTS"},"outputs":[],"source":["data_transform = transforms.Compose([\n","    transforms.Resize(size=(128,128), antialias=None), #resize image\n","    transforms.RandomHorizontalFlip(p=0.5), #flip the images randomly horizontally\n","    transforms.ToTensor()\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SznBYe2tYw_v"},"outputs":[],"source":["# Loading and transforming data using datasets.ImageFolder\n","train_data_cl = datasets.ImageFolder(root='add_your_training_path',\n","                                     transform=data_transform, #a transform for the data\n","                                     target_transform=None) #a transform for the label/target\n","val_data_cl = datasets.ImageFolder(root='add_your_validation_path',\n","                                   transform=data_transform)\n","train_data_cl, val_data_cl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNTKoRMrYE6S"},"outputs":[],"source":["import requests\n","\n","# Download helper functions from Learn PyTorch repo (if not already downloaded)\n","if os.path.isfile(\"helper_function_script_at_given_path.py\"):\n","  print(\"helper_functions.py already exists, skipping download\")\n","else:\n","  print(\"Downloading helper_functions.py\")\n","  os.chdir(\"given_path_for_helper_function_script\")\n","  # Note: you need the \"raw\" GitHub URL for this to work\n","  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","  with open(\"helper_functions.py\", \"wb\") as f:\n","    f.write(request.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSLPRVfwZnlI"},"outputs":[],"source":["# Create a new instance of TinyVGG (the same class as our saved state_dict())\n","# Note: loading model will error if the shapes here aren't the same as the saved version\n","loaded_model_for_LIME = TinyVGG(input_shape=3,\n","                                hidden_units=10, # try changing this to 128 and seeing what happens\n","                                output_shape=1).to(device)\n","# Load in the saved state_dict()\n","loaded_model_for_LIME.load_state_dict(torch.load(f=\"path_of_your_saved_CNN_model.pth\", map_location=torch.device(\"cpu\")))\n","\n","# send model to target device\n","loaded_model_for_LIME.to(device)"]},{"cell_type":"markdown","metadata":{"id":"03l_ozxDaysi"},"source":["## Load a sample image and view it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpSOMvl0Q9Sq"},"outputs":[],"source":["import os\n","import random\n","\n","def select_random_image(folder_path):\n","    # Get a list of all files in the folder\n","    all_files = os.listdir(folder_path)\n","\n","    # Filter the list to include only image files (you may need to adjust this based on your file types)\n","    image_files = [file for file in all_files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","    if not image_files:\n","        print(\"No image files found in the specified folder.\")\n","        return None\n","\n","    # Randomly select an image from the list\n","    selected_image = random.choice(image_files) #image_files[3]#\n","\n","    # Construct the full path to the selected image\n","    image_path = os.path.join(folder_path, selected_image)\n","\n","    return image_path, selected_image\n","\n","def plot_image(image_path,destination,selected_image):\n","    # Load and plot the image\n","    img = plt.imread(image_path)\n","    imgplot = plt.imshow(img)\n","    plt.axis('off')  # Turn off axis labels\n","    plt.savefig( os.path.join(destination, selected_image)+\"_OriginalPic.png\", bbox_inches='tight')\n","    plt.show()\n","\n","# Example usage:\n","folder_path = \"path_of_test_dataset_specific_class\"\n","random_image, selected_filename = select_random_image(folder_path)\n","destination_path = \"path_to_save_LIME_output\"\n","if random_image:\n","    print(f\"Randomly selected image: {random_image}\")\n","    plot_image(random_image,destination_path,selected_filename)\n","else:\n","    print(\"No image selected.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LAfyPN814Jd"},"outputs":[],"source":["def get_image(path):\n","  with open(os.path.abspath(path), 'rb') as f:\n","    with Image.open(f) as img:\n","      return img.convert('RGB')\n","# taking an image from the test set\n","img = get_image(random_image)\n","plt.imshow(img)\n","plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_SS7Tlsa-0r"},"outputs":[],"source":["# we will convert the above image to tensor and apply transformations similar to loaded model\n","def get_input_transform(target_size = (128,128)):\n","  return transforms.Compose([\n","      transforms.Resize(target_size),\n","      transforms.ToTensor(),\n","  ])\n","  return transf\n","def get_input_tensors(img):\n","  transf = get_input_transform()\n","  #unsqueeze converts single image to batch of 1\n","  return transf(img).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"Cvds8mCVbqRw"},"source":["### Load labels for prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bIg0IZsbk8d"},"outputs":[],"source":["class_info = {0:'crop', 1:'weed'}\n","class_info[1]"]},{"cell_type":"markdown","metadata":{"id":"rxVuxBdXb7_P"},"source":["### Get the prediction for the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZJ8hINab00z"},"outputs":[],"source":["img_t = get_input_tensors(img).to(device)\n","loaded_model_for_LIME.eval()\n","logits = loaded_model_for_LIME(img_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UExyHCDbnbV2"},"outputs":[],"source":["# Apply sigmoid activation function element-wise\n","probs = torch.sigmoid(logits)\n","probs"]},{"cell_type":"markdown","metadata":{"id":"DDmx76IxdpWb"},"source":["### LIME:\n","We are getting ready to use Lime. Lime produces the array of images from original input image by pertubation algorithm. So we need to provide two things: (1) original image as numpy array (2) classification function that would take array of purturbed images as input and produce the probabilities for each class for each image as output.\n","\n","For Pytorch, first we need to define two separate transforms: (1) to take PIL image, resize and crop it (2) take resized, cropped image and apply whitening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCChbeKQdDk5"},"outputs":[],"source":["def get_pil_transform():\n","    transf = transforms.Compose([\n","        transforms.Resize((128, 128)),\n","    ])\n","    return transf\n","\n","def get_preprocess_transform():\n","    transf = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","    return transf\n","\n","pill_transf = get_pil_transform()\n","preprocess_transform = get_preprocess_transform()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrszfh9Od2bu"},"outputs":[],"source":["pill_transf, preprocess_transform"]},{"cell_type":"markdown","metadata":{"id":"aJJlJUgCgInY"},"source":["Now we are ready to define classification function that Lime needs. The input to this function is numpy array of images where each image is ndarray of shape (channel, height, width). The output is numpy aaray of shape (image index, classes) where each value in array should be probability for that image, class combination.\n","\n","classifier prediction probability function, which takes a numpy array and outputs prediction probabilities. (LIME documentation)\n","\n","https://lime-ml.readthedocs.io/en/latest/lime.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHb9p9LId6Bk"},"outputs":[],"source":["def batch_predict(images):\n","  loaded_model_for_LIME.eval()\n","  batch = torch.stack(tuple(preprocess_transform(i) for i in images)) #dim=0)\n","\n","  #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  loaded_model_for_LIME.to(device)\n","  batch = batch.to(device)\n","\n","  logits = loaded_model_for_LIME(batch)\n","  probs_predict = torch.sigmoid(logits) #F.torch.sigmoid(logits)\n","  return probs_predict.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AUBrFclgl0T"},"outputs":[],"source":["# let's test our function on the sample image\n","test_pred = batch_predict([pill_transf(img)])"]},{"cell_type":"markdown","metadata":{"id":"TNfOOwb3hiz1"},"source":["Import LIME and create explanations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1-yq_D7hDU7"},"outputs":[],"source":["pip install lime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcBfoEG2hmup"},"outputs":[],"source":["from lime import lime_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brsS00QMdsdV"},"outputs":[],"source":["np.array(pill_transf(img)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlo_csyBhdr7"},"outputs":[],"source":["img_transformed = pill_transf(img)\n","print(type(img_transformed)) #he output of pill_transf(img) is a PIL Image object.\n","#In order to use LIME with this image data, you need to convert it to a NumPy array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IC_iJvN8iCBe"},"outputs":[],"source":["img_pil = pill_transf(img)\n","\n","# Convert the PIL image to NumPy array\n","img_array = np.array(img_pil)\n","\n","# Printing the datatype of img_array\n","print(type(img_array))"]},{"cell_type":"markdown","metadata":{"id":"d5JAyp8WuZFR"},"source":["**explain_instance:** Generates explanations for a prediction.\n","\n","First, we generate neighborhood data by randomly perturbing features from the instance (see __data_inverse). We then learn locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way (see lime_base.py). (LIME documentation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxLouAY9hp6k"},"outputs":[],"source":["explainer = lime_image.LimeImageExplainer() #creating an explainer object\n","explanation = explainer.explain_instance(img_array,#np.array(pill_transf(img)), # image instance you want to explain\n","                                         batch_predict, #classification function\n","                                         top_labels=1,\n","                                         hide_color=0,\n","                                         num_samples=1000 #number of images that will be sent to the classification function\n","                                         )\n","#neighborhood_labels = np.array(explanation.local_pred.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqocGy4hh8Fk"},"outputs":[],"source":["from skimage.segmentation import mark_boundaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVaz3OkR30we"},"outputs":[],"source":["temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=False)\n","fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].imshow(mark_boundaries(temp /255,np.flipud(mask)))\n","ax[0].axis('off')\n","ax[1].imshow(img_transformed) #this is pill transformed vs -- what is img_t??\n","ax[1].axis('off')\n","plt.savefig(os.path.join(destination_path, selected_filename)+\"_Explanation.png\", bbox_inches=\"tight\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2dDIiKL4mzN"},"outputs":[],"source":["temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, min_weight=0.0, num_features=10, hide_rest=True, negative_only=False)\n","img_boundary2 = mark_boundaries(temp/255, np.flipud(mask))\n","fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].imshow(img_boundary2)\n","#plt.imshow(img_boundary2)\n","ax[0].axis(\"off\")\n","ax[1].imshow(img_transformed)\n","ax[1].axis(\"off\")\n","plt.savefig(os.path.join(destination_path, selected_filename)+\"_Hide_rest_trueExplanation.png\", bbox_inches=\"tight\")\n","# output doesn't show red color\n","#i.e. area which is not contributing towards the prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U1h8Y-JPrSh"},"outputs":[],"source":["temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, min_weight=0.0, num_features=10, hide_rest=False, negative_only=True)\n","img_boundary2 = mark_boundaries(temp/255, np.flipud(mask))\n","fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].imshow(img_boundary2)\n","#plt.imshow(img_boundary2)\n","ax[0].axis(\"off\")\n","ax[1].imshow(img_transformed)\n","ax[1].axis(\"off\")\n","plt.savefig(os.path.join(destination_path, selected_filename)+\"_Hide_rest_false+ negative_only_true Explanation.png\", bbox_inches=\"tight\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9IJfmrWD-TQ"},"outputs":[],"source":["temp, mask = explanation.get_image_and_mask(0, positive_only=True, num_features=5, hide_rest=False)\n","plt.imshow(mark_boundaries(temp / 255 + 0.2, mask))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0f352OqEWMA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOEdaYzUCoh+t34tEC0LhmR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}