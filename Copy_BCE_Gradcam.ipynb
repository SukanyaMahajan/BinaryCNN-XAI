{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"135fHPJtkv0W-EOusZplwvWEQcwouxgEO","authorship_tag":"ABX9TyNJexxO3x+5ofvHVxkc5kTc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Model architecture-TinyVGG from:https://poloclub.github.io/cnn-explainer/"],"metadata":{"id":"-OOzA8xNHooE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wdg9lrrmFAdD"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch.nn as nn\n","import numpy as np\n","\n","from torchvision import models, transforms\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True) #use this if your dataset is on google drive"],"metadata":{"id":"4ozaEwWsoSSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"SdXkxY0soksL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TinyVGG(nn.Module):\n","  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","    super().__init__()\n","    self.conv_block_1 = nn.Sequential(\n","        nn.Conv2d(in_channels=input_shape,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(in_channels=hidden_units,\n","                  out_channels=hidden_units,\n","                  kernel_size=3,\n","                  stride=1,\n","                  padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2,\n","                     stride=2)\n","    )\n","    self.conv_block_2 = nn.Sequential(\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(in_features = hidden_units*32*32,\n","                  out_features=output_shape)\n","    )\n","  def forward(self, x:torch.Tensor):\n","    x = self.conv_block_1(x)\n","    x = self.conv_block_2(x)\n","    x = self.classifier(x)\n","    return x\n","\n","torch.manual_seed(42)\n","model_BCE_20epochs = TinyVGG(input_shape=3,\n","                  hidden_units=10,\n","                  output_shape=1).to(device)\n","model_BCE_20epochs"],"metadata":{"id":"UbGhGQw9wMKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transform = transforms.Compose([\n","    transforms.Resize(size=(128,128), antialias=None), #resize image\n","    transforms.RandomHorizontalFlip(p=0.5), #flip the images randomly horizontally\n","    transforms.ToTensor(),\n","])\n","#data_transform varible will be used in the following ImageFolder code's ."],"metadata":{"id":"rZnc8_Hnqdhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading and transforming data using datasets.ImageFolder\n","train_data_cl = datasets.ImageFolder(root='add_your_training_path',\n","                                     transform=data_transform, #a transform for the data\n","                                     target_transform=None) #a transform for the label/target\n","val_data_cl = datasets.ImageFolder(root='add_your_validation_path',\n","                                   transform=data_transform)\n","train_data_cl, val_data_cl"],"metadata":{"id":"tG-tSpzuqmbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new instance of TinyVGG (the same class as our saved state_dict())\n","# Note: loading model will error if the shapes here aren't the same as the saved version\n","loaded_model_for_gradcam = TinyVGG(input_shape=3,\n","                                   hidden_units=10, # try changing this to 128 and seeing what happens\n","                                   output_shape=1).to(device)\n","\n","# Load in the saved state_dict()\n","loaded_model_for_gradcam.load_state_dict(torch.load(f=\"path_of_your_saved_CNN_model.pth\", map_location=torch.device(\"cpu\")))\n","\n","# Send model to GPU\n","loaded_model_for_gradcam = loaded_model_for_gradcam.to(device)"],"metadata":{"id":"tMska3M3pPxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_for_gradcam"],"metadata":{"id":"pATG1peyJXBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_for_gradcam.eval()"],"metadata":{"id":"9CXGKzYqJZP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","# define the hooks\n","gradients = None\n","activations = None\n","\n","def backward_hook(module, grad_input, grad_output):\n","  global gradients\n","  print(\"Backward hook running...\")\n","  gradients = grad_output\n","  # print the size of the gradient\n","  print(f'Gradients size: {gradients[0].size()}')\n","\n","def forward_hook(module, args, output):\n","  global activations\n","  print(\"Forward hook running...\")\n","  activations = output\n","  # Print the size of the activations\n","  print(f'Activations size: {activations.size()}')"],"metadata":{"id":"G3rQkKmI_cJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the last convolutional layer in your model\n","# In your TinyVGG model(the loaded model), the last convolutional layer is in conv_block_2\n","last_conv_layer = loaded_model_for_gradcam.conv_block_2\n","last_conv_layer"],"metadata":{"id":"VbKdeiYB5KgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Register the backward hook on the last convolutional layer\n","backward_hook = last_conv_layer.register_full_backward_hook(backward_hook, prepend=False)\n","# Register the forward hook on the last convolutional layer\n","forward_hook = last_conv_layer.register_forward_hook(forward_hook, prepend=False)"],"metadata":{"id":"DSOEdyyA5Yu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","\n","def select_random_image(folder_path):\n","    # Get a list of all files in the folder\n","    all_files = os.listdir(folder_path)\n","\n","    # Filter the list to include only image files (you may need to adjust this based on your file types)\n","    image_files = [file for file in all_files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","    if not image_files:\n","        print(\"No image files found in the specified folder.\")\n","        return None\n","\n","    # Randomly select an image from the list\n","    #selected_image = image_files[213]\n","    selected_image = random.choice(image_files)\n","\n","    # Construct the full path to the selected image\n","    image_path = os.path.join(folder_path, selected_image)\n","\n","    return image_path, selected_image\n","\n","def plot_image(image_path, selected_image, destination):\n","    # Load and plot the image\n","    img = plt.imread(image_path)\n","    imgplot = plt.imshow(img)\n","    plt.axis('off')  # Turn off axis labels\n","\n","    plt.savefig(os.path.join(destination, selected_image)+\"_OriginalPic.png\", bbox_inches='tight')\n","    plt.show()\n","\n","# Example usage:\n","folder_path = \"path_of_test_dataset_specific_class\"\n","random_image, selected_image = select_random_image(folder_path)\n","destination = \"path_to_save_GradCam_output\"\n","if random_image:\n","    print(f\"Randomly selected image: {random_image}\")\n","    plot_image(random_image, selected_image, destination)\n","else:\n","    print(\"No image selected.\")"],"metadata":{"id":"YpSOMvl0Q9Sq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = Image.open(random_image).convert('RGB')\n","\n","transf = transforms.Compose([\n","    transforms.Resize((128,128), antialias=None),\n","    transforms.ToTensor(),\n","  ])\n","img_tensor = transf(image).to(device) #stores the tensor that represents the image"],"metadata":{"id":"MlhSmU6KAo1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_tensor"],"metadata":{"id":"7SNOfTylKiDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# applying the above transforms on various images.\n","import random\n","def plot_transformed_images(image_path, transform, n=4, seed=42):\n","\n","  \"\"\"Plots a series of random images from image_paths.\n","\n","    Will open n image paths from image_paths, transform them\n","    with transform and plot them side by side.\n","\n","    Args:\n","        image_paths (list): List of target image paths.\n","        transform (PyTorch Transforms): Transforms to apply to images.\n","        n (int, optional): Number of images to plot. Defaults to 3.\n","        seed (int, optional): Random seed for the random generator. Defaults to 42.\n","    \"\"\"\n","  #random.seed(seed)\n","  #random_image_paths = random.sample(image_path, k=n)\n","  #for image_path in random_image_paths:\n","\n","  with Image.open(image_path) as f:\n","    fig, ax = plt.subplots(1,2)\n","    ax[0].imshow(f)\n","    ax[0].set_title(f\"original \\nsize: {f.size}\")\n","    ax[0].axis(\"off\")\n","\n","    # Transform and plot image\n","    # Note: permute() will change shape of image to suit matplotlib\n","    # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n","    transformed_image = transform(f).permute(1,2,0)\n","    ax[1].imshow(transformed_image)\n","    ax[1].set_title(f\"Tranformed \\nsize: {transformed_image.shape}\")\n","    ax[1].axis(\"off\")\n","\n","    fig.suptitle(f\"class: {image_path}\", fontsize=16)\n","plot_transformed_images(image_path = random_image,\n","                        transform=transf,\n","                        n=4)\n"],"metadata":{"id":"djgP9GVMg_g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_tensor.shape"],"metadata":{"id":"gCO-94hdyY5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_tensor.unsqueeze(0).shape"],"metadata":{"id":"Sa1tKpS9wnWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_for_gradcam(img_tensor.unsqueeze(0))"],"metadata":{"id":"Kdszajl4CXfp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_for_gradcam(img_tensor.unsqueeze(0)).sum().backward()"],"metadata":{"id":"W8Waet_dvLWH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## -> Computing Grad-CAM"],"metadata":{"id":"nX_RyRPmLwO3"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","# pool the gradients across the channels\n","pooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])"],"metadata":{"id":"K9zUKawHLpJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weight the channels by corresponding gradients\n","for i in range(activations.size()[1]):\n","    activations[:, i, :, :] *= pooled_gradients[i]\n","\n","# average the channels of the activations\n","heatmap = torch.mean(activations,dim=1).squeeze()\n","\n","# relu on top of the heatmap\n","heatmap = F.relu(heatmap)\n","\n","# normalize the heatmap\n","heatmap /= torch.max(heatmap)\n","\n","# Move the heatmap tensor to the CPU before converting to NumPy otherwise it will give the following error:\n","#TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.**\n","heatmap = heatmap.cpu()\n","\n","# Now you can convert the heatmap tensor to a NumPy array\n","heatmap_numpy = heatmap.detach().numpy()\n","\n","# draw the heatmap\n","plt.matshow(heatmap.detach())\n","plt.savefig(os.path.join(destination, selected_image)+\"_HeatMap.png\", bbox_inches=\"tight\")"],"metadata":{"id":"j18LIcmCN9pW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## -> Overlapping heatap over the opened image"],"metadata":{"id":"a8k22kO3MCOl"}},{"cell_type":"code","source":["from torchvision.transforms.functional import to_pil_image\n","from matplotlib import colormaps\n","import numpy as np\n","import PIL\n","\n","# Define the extent based on the dimensions of the image\n","extent = (0,128, 128,0)  # Replace img_width and img_height with your image dimensions\n","# Define the extent (left, right, bottom, top)\n","#(0, 128, 0, 128): earlier extent values which were causing problem\n","# correct = (0, 128, 128,0)\n","\n","# Create a figure and plot the first image\n","fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].axis('off')\n","ax[1].axis(\"off\") # removes the axis markers\n","\n","# First plot the original image\n","\n","#ax.imshow(image.resize((128,128), resample=PIL.Image.BICUBIC))\n","\n","ax[0].imshow(to_pil_image(img_tensor, mode=\"RGB\")) #- TENSOR img - does not look like original image\n","\n","# Resize the heatmap to the same size as the input image and defines\n","# a resample algorithm for increasing image resolution\n","# we need heatmap.detach() because it can't be converted to numpy array while\n","# requiring gradients\n","overlay = to_pil_image(heatmap.detach(), mode='F').resize((128,128), resample=PIL.Image.BICUBIC)\n","\n","# Apply any colormap you want\n","cmap = colormaps['nipy_spectral']\n","overlay = (255 * cmap(np.asarray(overlay))[:, :, :3]).astype(np.uint8)\n","\n","# Plot the heatmap on the same axes,\n","# but with alpha < 1 (this defines the transparency of the heatmap)\n","ax[0].imshow(overlay, alpha=0.4, interpolation='nearest', extent=extent)\n","ax[1].imshow(to_pil_image(img_tensor, mode=\"RGB\"))\n","# Show the plot\n","plt.savefig(os.path.join(destination, selected_image)+\"_alpha 0.4-Explanation.png\", bbox_inches=\"tight\")\n","plt.show()"],"metadata":{"id":"1sR6s6U7O1D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].axis('off')\n","ax[1].axis(\"off\") # removes the axis markers\n","ax[0].imshow(to_pil_image(img_tensor, mode=\"RGB\")) #- TENSOR img - does not look like original image\n","\n","# Plot the heatmap on the same axes,\n","# but with alpha < 1 (this defines the transparency of the heatmap)\n","ax[0].imshow(overlay, alpha=0.2, interpolation='nearest', extent=extent)\n","ax[1].imshow(to_pil_image(img_tensor, mode=\"RGB\"))\n","# Show the plot\n","plt.savefig(os.path.join(destination, selected_image)+\"_alpha 0.2-Explanation.png\", bbox_inches=\"tight\")\n","plt.show()"],"metadata":{"id":"VBXTuJY58gM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2, figsize=(9,9))\n","ax[0].axis('off')\n","ax[1].axis(\"off\") # removes the axis markers\n","\n","ax[0].imshow(to_pil_image(img_tensor, mode=\"RGB\")) #- TENSOR img - does not look like original image\n","# Plot the heatmap on the same axes,\n","# but with alpha < 1 (this defines the transparency of the heatmap)\n","ax[0].imshow(overlay, alpha=0.6, interpolation='nearest', extent=extent)\n","ax[1].imshow(to_pil_image(img_tensor, mode=\"RGB\"))\n","# Show the plot\n","plt.savefig(os.path.join(destination, selected_image)+\"_alpha 0.6-Explanation.png\", bbox_inches=\"tight\")\n","plt.show()"],"metadata":{"id":"4wKqGSSq8r-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove the hooks to avoid interfering with future computations\n","forward_hook.remove()\n","backward_hook.remove()"],"metadata":{"id":"T3bxn-aXNxfQ"},"execution_count":null,"outputs":[]}]}